{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Lane Finding Project\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "\n",
    "* Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n",
    "* Apply a distortion correction to raw images.\n",
    "* Use color transforms, gradients, etc., to create a thresholded binary image.\n",
    "* Apply a perspective transform to rectify binary image (\"birds-eye view\").\n",
    "* Detect lane pixels and fit to find the lane boundary.\n",
    "* Determine the curvature of the lane and vehicle position with respect to center.\n",
    "* Warp the detected lane boundaries back onto the original image.\n",
    "* Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.\n",
    "\n",
    "---\n",
    "## First, I'll compute the camera calibration using chessboard images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 1280)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1868973603423751 [[  1.15396093e+03   0.00000000e+00   6.69705357e+02]\n [  0.00000000e+00   1.14802496e+03   3.85656234e+02]\n [  0.00000000e+00   0.00000000e+00   1.00000000e+00]] [[ -2.41017956e-01  -5.30721171e-02  -1.15810354e-03  -1.28318858e-04\n    2.67125300e-02]] [array([[-0.19458111],\n       [-0.76190721],\n       [ 0.12161603]]), array([[ 0.17850283],\n       [-0.06187035],\n       [ 0.00114955]]), array([[ 0.05531879],\n       [-0.51892709],\n       [-0.00398629]]), array([[-0.02434082],\n       [-0.48814782],\n       [ 0.0205404 ]]), array([[-0.33084415],\n       [ 0.65914967],\n       [-0.41537657]]), array([[ 0.02790357],\n       [-0.70679126],\n       [-0.01797426]]), array([[ 0.04125139],\n       [-0.46371786],\n       [-0.05619974]]), array([[ 0.0178336 ],\n       [ 0.63674357],\n       [ 0.00826376]]), array([[-0.44823487],\n       [-0.06589252],\n       [-0.01933811]]), array([[ 0.50716545],\n       [-0.22122219],\n       [ 0.02988717]]), array([[-0.02563264],\n       [ 0.38170173],\n       [-0.00443487]]), array([[ 0.01805198],\n       [ 0.02669678],\n       [-0.00551439]]), array([[ 0.08447458],\n       [ 0.38039934],\n       [ 0.05427302]]), array([[ 0.63236599],\n       [-0.04827079],\n       [ 0.01721011]]), array([[ 0.03492402],\n       [ 0.45739729],\n       [ 0.00492867]]), array([[ 0.03471112],\n       [ 0.65086688],\n       [ 0.00971071]]), array([[ 0.21226762],\n       [-0.067192  ],\n       [ 0.0124846 ]])] [array([[ -0.87749794],\n       [ -4.5628898 ],\n       [ 21.64088689]]), array([[ -3.63923607],\n       [ -4.12214649],\n       [ 17.72145269]]), array([[  5.35073144],\n       [ -4.43439198],\n       [ 20.69860635]]), array([[  4.46791786],\n       [ -1.45553778],\n       [ 19.91167384]]), array([[ -6.06136681],\n       [ -1.54277113],\n       [ 26.6077651 ]]), array([[  0.7014351],\n       [ -2.9001952],\n       [ 19.5033317]]), array([[  4.86295442],\n       [ -5.0332483 ],\n       [ 19.69539304]]), array([[ -9.64623426],\n       [ -3.25401813],\n       [ 32.03535572]]), array([[ -4.40335216],\n       [ -3.04835705],\n       [ 10.68886726]]), array([[ -2.11922855],\n       [ -0.71465388],\n       [ 19.53183554]]), array([[-13.43070279],\n       [ -0.45669297],\n       [ 24.41876447]]), array([[ -4.97952826],\n       [ -3.83821782],\n       [ 30.41248115]]), array([[-13.0435234 ],\n       [ -5.57212836],\n       [ 23.64304788]]), array([[-3.83498058],\n       [-1.59623848],\n       [ 7.94136389]]), array([[-16.99963028],\n       [ -3.45711062],\n       [ 31.88140225]]), array([[ -0.21898193],\n       [ -3.45905797],\n       [ 21.82352738]]), array([[ -3.98286443],\n       [ -1.31029172],\n       [ 16.98785567]])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt\n",
    "\n",
    "# prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "objp = np.zeros((6*9,3), np.float32)\n",
    "objp[:,:2] = np.mgrid[0:9,0:6].T.reshape(-1,2)\n",
    "\n",
    "# Arrays to store object points and image points from all the images.\n",
    "objpoints = [] # 3d points in real world space\n",
    "imgpoints = [] # 2d points in image plane.\n",
    "\n",
    "# Make a list of calibration images\n",
    "images = glob.glob('camera_cal/calibration*.jpg')\n",
    "print(len(images))\n",
    "# Step through the list and search for chessboard corners\n",
    "gray = []\n",
    "for fname in images:\n",
    "    img = cv2.imread(fname)\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Find the chessboard corners\n",
    "    ret, corners = cv2.findChessboardCorners(gray, (9,6),None)\n",
    "\n",
    "    # If found, add object points, image points\n",
    "    if ret == True:\n",
    "        objpoints.append(objp)\n",
    "        imgpoints.append(corners)\n",
    "\n",
    "        # Draw and display the corners\n",
    "        #img = cv2.drawChessboardCorners(img, (9,6), corners, ret)\n",
    "        #cv2.imshow('img',img)\n",
    "        #cv2.waitKey(500)\n",
    "print (gray.shape)       \n",
    "ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)\n",
    "print (ret, mtx, dist, rvecs, tvecs)\n",
    "#img = cv2.imread('test_images/test1.jpg')\n",
    "\n",
    "#dst = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "\n",
    "\n",
    "#cv2.imshow('img',img)\n",
    "#cv2.imshow('img2',dst)\n",
    "#cv2.waitKey()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And so on and so forth..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image is: <class 'numpy.ndarray'> with dimensions: (720, 1280, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soumya/.conda/envs/carnd-term1/lib/python3.5/site-packages/matplotlib/backend_bases.py:2445: MatplotlibDeprecationWarning: Using default event loop until function specific to this GUI is implemented\n  warnings.warn(str, mplDeprecation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region of interest [[[  10  720]\n  [ 640  417]\n  [ 640  417]\n  [1270  720]]] <class 'numpy.ndarray'> (720, 1280)\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "/home/travis/miniconda/conda-bld/conda_1486587071158/work/opencv-3.1.0/modules/core/src/arithm.cpp:639: error: (-209) The operation is neither 'array op array' (where arrays have the same size and the same number of channels), nor 'array op scalar', nor 'scalar op array' in function arithm_op\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a410de23f418>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m '''\n\u001b[1;32m    113\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_images/test1.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m \u001b[0mprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-a410de23f418>\u001b[0m in \u001b[0;36mprocess_image\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'region of interest'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mmasked_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0moutImage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweighted_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutImage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'outImage'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-a410de23f418>\u001b[0m in \u001b[0;36mweighted_img\u001b[0;34m(img, initial_img, α, β, λ)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mNOTE\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minitial_img\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mimg\u001b[0m \u001b[0mmust\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;31m!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \"\"\"\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddWeighted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mα\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mβ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mλ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: /home/travis/miniconda/conda-bld/conda_1486587071158/work/opencv-3.1.0/modules/core/src/arithm.cpp:639: error: (-209) The operation is neither 'array op array' (where arrays have the same size and the same number of channels), nor 'array op scalar', nor 'scalar op array' in function arithm_op\n"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "def region_of_interest(img, vertices):\n",
    "    \"\"\"\n",
    "    Applies an image mask.\n",
    "    \n",
    "    Only keeps the region of the image defined by the polygon\n",
    "    formed from `vertices`. The rest of the image is set to black.\n",
    "    \"\"\"\n",
    "    #defining a blank mask to start with\n",
    "    mask = np.zeros_like(img)   \n",
    "    #plt.imshow(img)\n",
    "    #plt.waitforbuttonpress()\n",
    "    #defining a 3 channel or 1 channel color to fill the mask with depending on the input image\n",
    "    if len(img.shape) > 2:\n",
    "        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image\n",
    "        ignore_mask_color = (255,) * channel_count\n",
    "    else:\n",
    "        ignore_mask_color = 255\n",
    "        \n",
    "    #filling pixels inside the polygon defined by \"vertices\" with the fill color    \n",
    "    cv2.fillPoly(mask, vertices, ignore_mask_color)\n",
    "    \n",
    "    #returning the image only where mask pixels are nonzero\n",
    "    masked_image = cv2.bitwise_and(img, mask)\n",
    "    return masked_image\n",
    "\n",
    "def weighted_img(img, initial_img, α=0.8, β=1., λ=0.):\n",
    "    \"\"\"\n",
    "    `img` is the output of the hough_lines(), An image with lines drawn on it.\n",
    "    Should be a blank image (all black) with lines drawn on it.\n",
    "    \n",
    "    `initial_img` should be the image before any processing.\n",
    "    \n",
    "    The result image is computed as follows:\n",
    "    \n",
    "    initial_img * α + img * β + λ\n",
    "    NOTE: initial_img and img must be the same shape!\n",
    "    \"\"\"\n",
    "    return cv2.addWeighted(initial_img, α, img, β, λ)\n",
    "\n",
    "def process_image(img):\n",
    "    \n",
    "    #printing out some stats and plotting\n",
    "    print('This image is:', type(img), 'with dimensions:', img.shape)\n",
    "    # if you wanted to show a single color channel image called 'gray', for example, call as plt.imshow(gray, cmap='gray')\n",
    "    image = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "\n",
    "    hls = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n",
    "    S = hls[:,:,2]\n",
    "    thresh = (90, 255)\n",
    "    s_binary = np.zeros_like(S)\n",
    "    s_binary[(S > thresh[0]) & (S <= thresh[1])] = 1\n",
    "    \n",
    "    \n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)    \n",
    "    thresh_min = 20\n",
    "    thresh_max = 100\n",
    "    \n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1,0, ksize = 3) # Take the derivative in x\n",
    "    abs_sobelx = np.absolute(sobelx) # Absolute x derivative to accentuate lines away from horizontal\n",
    "    scaled_sobel = np.uint8(255*abs_sobelx/np.max(abs_sobelx))\n",
    "\n",
    "    #sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    # Calculate the gradient magnitude\n",
    "    #gradmag = np.sqrt(sobelx**2 + sobely**2)\n",
    "    # Rescale to 8 bit\n",
    "    #scale_factor = np.max(gradmag)/255 \n",
    "    #gradmag = (gradmag/scale_factor).astype(np.uint8) \n",
    "    # Create a binary image of ones where threshold is met, zeros otherwise\n",
    "    #sgrad_binary = np.zeros_like(gradmag)\n",
    "    \n",
    "    #sgrad_binary[(gradmag >= thresh_min) & (gradmag <= thresh_max)] = 1\n",
    "    \n",
    "    \n",
    "    sxbinary = np.zeros_like(scaled_sobel)\n",
    "    sxbinary[(scaled_sobel >= thresh_min) & (scaled_sobel <= thresh_max)] = 1\n",
    "    combined_binary = np.zeros_like(sxbinary)\n",
    "    combined_binary[(s_binary == 1) | (sxbinary == 1)] = 1\n",
    "\n",
    "\n",
    "    #mask your edges and \n",
    "    imshape = combined_binary.shape\n",
    "    vertices = np.array([[(10,imshape[0]),(imshape[1]/2, imshape[0]*0.58),\n",
    "                          (imshape[1]/2, imshape[0]*0.58), (imshape[1]-10,imshape[0])]], dtype=np.int32)\n",
    "\n",
    "    masked_image = region_of_interest(combined_binary, vertices) \n",
    "    plt.imshow(masked_image)\n",
    "    plt.title('masked_image')\n",
    "    plt.waitforbuttonpress()\n",
    "    \n",
    "    print('region of interest', vertices, type(masked_image), masked_image.shape)\n",
    "    masked_image = np.expand_dims(masked_image, axis=0)\n",
    "    outImage = weighted_img(masked_image, image)\n",
    "    plt.imshow(outImage)\n",
    "    plt.title('outImage')\n",
    "    plt.waitforbuttonpress()\n",
    "    '''\n",
    "    #Define the Hough transform parameters\n",
    "    # Make a blank the same size as our image to draw on\n",
    "    rho = 1# distance resolution in pixels of the Hough grid\n",
    "    theta = np.pi/180 # angular resolution in radians of the Hough grid\n",
    "    threshold = 14     # minimum number of votes (intersections in Hough grid cell)\n",
    "    min_line_length = 5 #minimum number of pixels making up a line\n",
    "    max_line_gap = 180\n",
    "    # maximum gap in pixels between connectable line segments\n",
    "\n",
    "    line_image = hough_lines (masked_image, rho, theta, threshold, min_line_length, max_line_gap)\n",
    "\n",
    "    #plt.imshow(line_image)\n",
    "\n",
    "    outImage = weighted_img(line_image, image)\n",
    "    return outImage\n",
    "'''\n",
    "img = cv2.imread('test_images/test1.jpg')\n",
    "process_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'white_output = \\'test_videos_output/solidWhiteRight.mp4\\'\\nclip1 = VideoFileClip(\"test_videos/solidWhiteRight.mp4\")\\nwhite_clip = clip1.fl_image(process_image) #NOTE: this function expects color images!!\\n%time white_clip.write_videofile(white_output, audio=False)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''white_output = 'test_videos_output/solidWhiteRight.mp4'\n",
    "clip1 = VideoFileClip(\"test_videos/solidWhiteRight.mp4\")\n",
    "white_clip = clip1.fl_image(process_image) #NOTE: this function expects color images!!\n",
    "%time white_clip.write_videofile(white_output, audio=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
